<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta
      name="description"
      content="SeqDG introduces a sequence-based reconstruction objective to improve domain generalization in Egocentric Action Recognition."
    />
    <meta
      property="og:title"
      content="Domain Generalization using Action Sequences for Egocentric Action Recognition"
    />
    <meta
      property="og:description"
      content="SeqDG introduces a sequence-based reconstruction objective to improve domain generalization in Egocentric Action Recognition."
    />
    <meta property="og:url" content="Ashayan97.github.io/SeqDG" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/teaser.png" />
    <!--meta property="og:image:width" content="1200"/-->
    <!--meta property="og:image:height" content="630"/-->

    <meta
      name="twitter:title"
      content="Domain Generalization using Action Sequences for Egocentric Action Recognition"
    />
    <meta
      name="twitter:description"
      content="SeqDG introduces a sequence-based reconstruction objective to improve domain generalization in Egocentric Action Recognition."
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/image/teaser.png" />
    <!-- Keywords for your paper to be indexed by-->
    <meta
      name="keywords"
      content="Domain Generalization; Egocentric Vision; Computer Vision; Deep Learning"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>
      Domain Generalization using Action Sequences for Egocentric Action Recognition
    </title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.png" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <script async data-id="101477061" src="//static.getclicky.com/js"></script>

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="static/js/jquery.min.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <style type="text/css"></style>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title" style="font-size: 2.75rem;">
                Domain Generalization using Action Sequences for Egocentric Action Recognition
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block"
                  ><a
                    href="https://ashayan97.github.io"
                    target="_blank"
                    >Amirshayan Nasirimajd</a
                  >,</span
                >
                <span class="author-block"
                  ><a
                    href="https://chiaraplizz.github.io"
                    target="_blank"
                    >Chiara Plizzari</a
                  >,</span
                >
                <span class="author-block"
                  ><a
                    href="https://sapeirone.github.io"
                    target="_blank"
                    >Simone Alberto Peirone</a
                  >,</span
                >
                <br>
                <span class="author-block"
                  ><a
                    href="https://marcociccone.github.io"
                    target="_blank"
                    >Marco Ciccone</a
                  >,</span
                >
                <span class="author-block"
                  ><a
                    href="https://www.giuseppeaverta.me/"
                    target="_blank"
                    >Giuseppe Averta</a
                  >,</span
                >
                <span class="author-block"
                  ><a
                    href="https://vandal.polito.it/index.php/people/index.php/people/barbaracaputo/"
                    target="_blank"
                    >Barbara Caputo</a
                  ></span
                >
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  Politecnico di Torino&nbsp;&nbsp;
                  <br />
                  <small><tt>amirshayan.nasirimajd@studenti.polito.it</tt></small>
                  <br />
                  <b>Pattern Recognition Letters (June 2025)</b>
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->

                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2506.17685"
                      target="_blank"
                      title="Link to ArXiv paper"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>ArXiv</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a
                      href="https://www.sciencedirect.com/science/article/pii/S0167865525002387"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a
                      href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865525002387-mmc1.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary Materials</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/Ashayan97/SeqDG"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
    <section class="hero teaser">
      <div
        class="container is-max-desktop"
        style="width: calc(100% - 32px); max-width: 720px; margin-bottom: 32px"
      >
        <div
          class="hero-body"
          style="padding-bottom: 0.75rem; padding-top: 0.25rem"
        >
          <figure class="image" style="margin-top: 0.25rem; margin-bottom: 32px">
            <img
              src="static/images/teaser.png"
              alt=""
              style="max-width: 768px; margin: auto"
            />
          </figure>
          <p style="text-align: center;">
            Egocentric Action Recognition (EAR) models struggle in classifying the same action when it is performed in different environments (top). Despite different visual contexts, the sequence of actions is the same. Thus, considering previous and subsequent actions within a sequence proves beneficial for improving cross-domain robustness of the EAR models.
          </p>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Recognizing human activities from visual inputs, particularly through a first-person viewpoint, is essential for enabling robots to replicate human behavior. Egocentric vision, characterized by cameras worn by observers, captures diverse changes in illumination, viewpoint, and environment. This variability leads to a notable drop in the performance of Egocentric Action Recognition models when tested in environments not seen during training. In this paper, we tackle these challenges by proposing a domain generalization approach for Egocentric Action Recognition. Our insight is that action sequences often reflect consistent user intent across visual domains. By leveraging action sequences, we aim to enhance the model’s generalization ability across unseen environments. Our proposed method, named SeqDG, introduces a visual-text sequence reconstruction objective (SeqRec) that uses contextual cues from both text and visual inputs to reconstruct the central action of the sequence. Additionally, we enhance the model’s robustness by training it on mixed sequences of actions from different domains (SeqMix). We validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement in cross-domain action recognition in unseen environments, and on EGTEA the model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action recognition.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <section
      class="section hero"
      style="padding-bottom: 0.75rem; padding-top: 1.5rem"
    >
      <div class="container is-max-desktop">
        <div
          class="hero-body"
          style="padding-bottom: 0.75rem; padding-top: 1.5rem"
        >
          <h3 class="title is-3">The SeqDG Architecture</h3>
          <p>
            SeqDG is a domain generalization framework for egocentric action recognition that leverages the consistency of action sequences across different environments. The model takes both visual inputs (video clips) and textual narrations (e.g., verb-noun pairs) for a sequence of actions surrounding a central action. Visual features are encoded using a transformer encoder with a classification token, while textual features are extracted using a pre-trained language model. The central action in the sequence is masked in both modalities, and two separate decoders with cross-modal attention reconstruct the missing visual and textual information — a process that encourages the model to capture temporal and semantic dependencies across the sequence. 
          </p>
          <figure class="image" style="margin-top: 16px; margin-bottom: 32px">
            <img
              src="static/images/arch.png"
              alt=""
              style="max-width: 768px; margin: auto"
            />
          </figure>
          <h4 class="title is-4">Mixing action sequences across domains</h4>
          <p>
            SeqDG includes a sequence-mixing augmentation strategy (SeqMix), where actions with the same label but from different domains are combined to improve robustness to domain shifts. The final action classification is performed using the CLS token from the visual encoder, and the model is jointly trained with classification and reconstruction losses. During inference, only visual inputs are used.
          </p>
          <figure class="image" style="margin-top: 16px; margin-bottom: 32px">
            <img
              src="static/images/seqmix.png"
              alt=""
              style="max-width: 768px; margin: auto"
            />
            <caption >
              <p style="text-align: center;">
              Given a sequence from domain <i>d<sub>i</sub></i>, we replace action <i>a<sub>i</sub></i> in the sequence with a different action <i>a<sub>j</sub></i> from another sequence belonging to a different domain <i>d<sub>j</sub></i>.
              </p>
            </caption>
          </figure>
        </div>
      </div>
    </section>

    <section
      class="section hero"
      style="padding-bottom: 0.75rem; padding-top: 1.5rem"
    >
      <div class="container is-max-desktop">
        <div
          class="hero-body"
          style="padding-bottom: 0.75rem; padding-top: 1.5rem"
        >
          <h3 class="title is-3">Experimental results</h3>
          <p>
            We validate SeqDG on the EPIC-KITCHENS-100 UDA benchmark, comparing both UDA and DG methods in the Cross-Domain setting (target val. set). Models are evaluated in terms of Top-1 and Top-5 Verb, Noun and Action accuracy (%).
          </p>
          <figure class="image" style="margin-top: 16px; margin-bottom: 16px">
            <img
              src="static/images/ek100_sota.png"
              alt="Domain generalization results on EK100"
              style="margin: auto; margin-bottom: 4px"
            />
          </figure>
        </div>
      </div>
    </section>

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">Cite us</h2>
        <pre><code>
@article{NASIRIMAJD2025,
title = {Domain generalization using action sequences for egocentric action recognition},
journal = {Pattern Recognition Letters},
year = {2025},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2025.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167865525002387},
author = {Amirshayan Nasirimajd and Chiara Plizzari and Simone Alberto Peirone and Marco Ciccone and Giuseppe Averta and Barbara Caputo}
}
        </code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page. You are free to borrow the of this website, we
                just ask that you link back to this page in the footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
